{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Computer Vision - TÃ¼bingen (2021)**\n",
    "\n",
    "These are my notes from Andreas Geiger's Computer Vision course on Youtube."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Lecture 2. Image Formation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Photometric Image Formation**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $p \\in \\mathbb{R}^3$ be a 3D surface point and $v \\in \\mathbb{R}^3$ a viewing direction. Suppose that $r \\in \\mathbb{R}^3$ is the incoming light direction. Then the **rendering equation** describes how much of the light arriving at $p$ is reflected into viewing direction $v$ $$L_{out}(p,v ; \\lambda) = L_{emit}(p,v ; \\lambda) + \\int_{\\Omega} \\verb|BRDF|(p,r,v ; \\lambda) \\cdot L_{in}(p, r ; \\lambda) \\cdot n^T r dr$$ where $\\Omega$ is the unit hemisphere at normal $n$. Then $L_{emit}$ term is positive when the point also emits light. The bidirectional reflectance distribution function $\\verb|BRDF|$ defines how light is reflected at an opaque surface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typical BRDFs have a *diffuse* component that scatters light uniformly in all directions and leads to shading (smooth variation of intensity w.r.t the surface normal) and a *specular* component that depends strongly on the outgoing direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelling one light bounce is insufficient for rendering complex scenes. Global illumination techniques also take indirect illumination into account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Lecture 3. Structure from Motion**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Preliminaries**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SIFT constructs a scale space by iteratively filtering the image with a Gaussian where adjacent scales are subtracted, yielding a *difference of gaussians*. Interest points are detected as extrema in the resulting scale space. These are points where all neighboring points are higher (or lower for a minima) including neighbors at a different scale level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SIFT rotates the descriptor to align with the dominant gradient orientation. Then gradient histograms are computed for local sub-regions of the descriptor and all the histograms are concatenated and normalized to form a 128 dimensional feature vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature correspondences are retrieved with efficient nearest neighbor search and ambiguous matches are filtered by computing the ratio of distance from the closest neighbor to the distance of the second closest. A large ratio $(> 0.8)$ indicates that the found match might not be correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Two-frame Structure-from-Motion**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relation between camera pose and 3D structure from two image correspondences is described by *epipolar geometry*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main crux of this is the epipolar equation $$x_2^T E x_1 = x_2^T [t]_\\times R x_1 = 0$$ where $x_1,x_2$ are the observed coordinates in the two images after undoing the effect of the intrinsic camera matrix $K$ and $y \\to Ry + t$ is the change of 3D coordinates from first camera center to the second. The matrix $E$ is an example of an *essential matrix*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    " The epipolar line $l_2 = Ex_1$ in the second image passes through the second epipole, and so we have $e_2^T E x_1 = 0$. This is true for any $x_1$, so we must have $e_2^T E_1 = 0$. Similarly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can recover the essential matrix from $N$ image correspondences and we use the SVD to constrain the scale. It is important to whiten the image coordinates to have zero mean  and unit variance before performing this algorithm. The matrix recovered is then transformed appropriately to account for this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From $E$ we can recover the direction $\\hat{t}$ of the translation vector $t$ since we have $t^T E = 0$. Essential matrices have 5 DoF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the camera calibration $K_i$ is unknown, we obtain the criterion $$x_2^T K_2^{-T} E K_1^{-1} x_1 = 0 = x_2^T F x_1$$ where $F = K_2^{-T} E K_1^{-1}$ is the fundamental matrix. However, the intrinsic parameters cannot be directly determined and we obtain only a perspective reconstruction. If there is additional information like vanishing points, constancy of K across time, zero skew, aspect ratio etc., this can be upgraded to a metric reconstruction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have recovered point correspondences, we would like to recover the 3d coordinates. Write $x_i = P_i X$ so that $[x_i]_\\times P_i X = 0$. Since $[x_i]_\\times = \\begin{bmatrix} 0 & -1 & y_i \\\\ 1 & 0 & -x_1 \\\\ -y_i & x_i & 0  \\end{bmatrix}$. Thus, if $P_i  [p_i^1|p_i^2|p_i^3]^T$, we have $$[x_i]_\\times P_i =  \\begin{bmatrix} -p_i^2 + y_ip_i^3 \\\\ p_i^1  -x_1p_i^3 \\\\-y_ip_i^1 + x_ip_i^2  \\end{bmatrix} $$ The third equation is redundant, so we have $$A_i X = \\begin{bmatrix} -p_i^2 + y_ip_i^3 \\\\ p_i^1  -x_1p_i^3   \\end{bmatrix} X = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While DLT works well, it is not invariant to perspective transformations. The gold standard is to minimize the re-projection error $$X^* = \\arg\\min_{X} \\sum_{i=1}^{N} \\lvert\\lvert P_i(X) - x_i \\rvert\\rvert^2$$ using techniques like Levenberg-Marquardt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Bundle Adjustment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\Pi = \\{\\pi_i\\}$ denote the $N$ camera matrices, $\\mathcal{X}_w = \\{X_p\\}$ be the set of 3D points in world coordinates. Let $\\mathcal{X}_s = \\{x_{i,p}\\}$ be the images of the points in the corresponding camera. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bundle adjustment minimizes the re-projection error $$\\Pi^*, \\mathcal{X}_w^* = \\arg\\min_{\\pi, \\mathcal{X}_w} \\sum_{i=1}^{N} \\sum_{p=1}^{P} W_{f,p} \\lvert\\lvert x_{i,p} - \\pi_i(X_p) \\rvert\\rvert^2$$ Here $W_{i,p} = 1$ exactly when point $p$ is observed in image $i$ and $\\pi_i(X_p)$ is the de-homogenized point (we divide by the third coordinate). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modern Structure from motion approaches perform *incremental bundle adjustment* where a carefully selected two-view reconstruction is used for initialization and iteratively new images/cameras re added to the reconstruction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The issues with bundle adjustment are:\n",
    "\n",
    "1. It is a non-convex optimization problem and needs a good initialization to avoid getting trapped in bad local minima.\n",
    "2. Initializing all 3D points and cameras jointly is difficult, and so incremental bundle adjustment iteratively adds new images or cameras.\n",
    "3. Since there can be up to millions of features and thousands of cameras, large scale bundle adjustment is computationally demanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily the problem is usually sparse in practice and efficient sparse implementations can be used in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Incremental Structure-from-Motion**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two stages in the COLMAP pipeline:\n",
    "\n",
    "1. First robust 2d features are found and matched across images.\n",
    "2. Incrementally cameras are added after starting with two views."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at the Image registration step in a bit more detail.Suppose that we have a new image with correspondences to the current set $X_i, x_i$. Given a new image, new correspondences are triangulated and then a local bundle adjustment is performed on only the locally connected images (global bundle adjustemnt is performed only once in a while for efficiency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a reconstruction, we can obtain a dense reconstruction using epipolar geometry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Lecture 4: Stereo Reconstruction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Preliminaries**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Disparity* is inverse depth, the relative displacement between pixels of two images of the same scene. We wish to recover disparity for each pixel from two images and obtain a dense 3D model from 2 images of a static scene. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline for Multi-view Stereo is:\n",
    "\n",
    "1. Calibrate cameras.\n",
    "2. Rectify images given the calibration.\n",
    "3. Compute the disparity map for the reference image.\n",
    "4. Remove outliers using consistency/occlusion tests.\n",
    "5. Obtain depth from disparity using camera calibration.\n",
    "6. Construct a 3D model via techniques like volumetric fusing and meshing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline for 3D reconstruction is:\n",
    "\n",
    "1. Take input images and compute camera poses.\n",
    "2. Use camera poses to compute dense correspondences.\n",
    "3. Use the dense correspondences to form depth maps.\n",
    "4. Use depth maps with a depth fusion method to obtain a 3D reconstruction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have two images of a 3D point $x_1,x_2$, we have $x_2^T E x_1 = 0$. The correspondence point $x_2$ of a pixel $x_1$ lies on the epipolar line $Ex_1$ and the correspondence of a pixel $x_2$ in the second image lies on $E^T x_2$. This means we only need to search for the correspondence point on the epipolar line. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If both cameras face in the same direction (no rotation) and the translation vector is parallel to the image planes, the epipoles will lie at infinity. The epipolar lines will be parallel to the translation vector between the two images and so if we adjust for this, we will only have to look for correspondences using a *horizontal line search*. This simplies the implementation of this method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even if the images are not in the required setup, we can warp them through a rotation that maps both the image planes to a common plane parallel to the baseline. This is called *rectification* and is possible even without knowing a 3D reconstruction of the images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first assume $K_1 = K_2 = R = I$ and $\\mathbf{t} = (t,0,0)$. In this case the essential matrix is given by $$E = [\\mathbf{t}]_\\times R = \\begin{bmatrix} 0 & 0  & 0 \\\\ 0 & 0 & -t \\\\ 0 & t  & 0\\end{bmatrix}$$ This means the epipolar constraint becomes $x_2^T E x_1 = 0 = -t y_2 + ty_1$. Thus, $y_1 = y_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to find a rectifying rotation $R_{\\text{rect}} = [r_1, r_2, r_3]^T$, where $r_1 = \\frac{t}{\\lvert\\lvert t \\rvert\\rvert}$, $r_2 = [0,0,1]^T \\times r_1$ and $r_3 = r_1 \\times r_2$. The reason for $r_2$'s slightly strange definition is so that the $z$-direction is preserved.\n",
    "\n",
    "Then we have $R_{\\text{rect}} \\frac{t}{\\lvert\\lvert t\\rvert\\rvert} =R_{\\text{rect}} r_1 = [1,0,0]^T$, just as we require. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the rectification algorithm is:\n",
    "\n",
    "1. Estimate $E$ and decompose to $\\mathbf{t}$ and $R. \n",
    "2. Construct $R_{\\text{rect}}$.\n",
    "3. Warp pixels in the first image as $x_1' = KR_{rect} K_1^{-1} x_1$, and\n",
    "4. Warp pixels in the second image as $x_2 = K R R_{rect} K_2^{-1} x_2$,\n",
    "   \n",
    "where $K$ is a shared projection matrix that can be chosen arbitrarily (for example, $K = K_1$).\n",
    "\n",
    "In practice the inverse of the above transformations is applied on destination pixels to query the pixel which is warped to it. This will not land exactly in an integer location and some sort of bilinear or cubic interpolation is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relative horizontal displacement of points of different depths is called **disparity** and it is inversely proportional to depth. We want to recover depth from estimated disparity.\n",
    "\n",
    "Let $z$ be the true depth of a point, $b$ the baseline (the distance between camera centers after rectification), $d$ the disparity, and $f$ the focal length, we have $$\\frac{z-f}{b - d} = \\frac{z}{b}$$ by a simple similarity relation. This means $z = \\frac{fb}{d}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Block Matching**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To identify the disparity between two images we will need to determine if two image points correspond and even what it means for this to be so. This is complicated by the fact that a single pixel doesn't reveal local structure, and even a small region around a point of interest can look very different due to changes in perspective or illumination. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When performing block matching we query a point in the input image and sweep the horizontal direction and compare patches. We only need to do this to the left of the corresponding point in the right image because disparity has to be positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say we have $K \\times K$ windows of pixels flattened to $w_L, w_R \\in \\mathbb{R}^{K^2}$. We want to compare these patches, and a common method is Zero Normalized Cross-Correlation (ZNCC) $$\\verb|NCC|(x,y,d) = \\frac{\\left(w_L(x,y) - \\overline{w_L}(x,y)\\right)^T(w_r(x-d,y) - \\overline{w_r(x-d,y)})}{\\lvert\\lvert w_L(x,y) - \\overline{w_L}(x,y) \\rvert\\rvert \\,\\cdot\\, \\lvert\\lvert w_R(x-d,y) - \\overline{w_R}(x-d,y) \\rvert\\rvert}$$ Another common method is the sum of squared difference (SSD)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the block matching algorithm performs the following steps:\n",
    "\n",
    "1. Choose a disparity range $[0,D]$.\n",
    "2. For all pixels $x$ compute the best disparity (winner takes all)\n",
    "3. Do this for both images and apply left-right constituency check to remove outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When looking at the results of the block matching algorithm we see artifacts around the edges of the objects. These are caused by half occlusions where the patch is occluded in one of the images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Block matching assumes that all the pixels inside a window are displaced by the same disparity $d$. This is called the *fronto-parallel assumption* and is often invalid. Slanted surfaces deform perspectively when the viewpoint changes. Also, the window content changes differently at discontinuities of disparity.\n",
    "\n",
    "So there is a tradeoff between small window sizes that lead to matching ambiguities and noise in the disparity map and large window sizes that lead to smoother results but loss of details and border bleeding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can detect for outliers and half-occlusions can be detected via a left-right consistency test. For every pixel in the left image we compute the disparity and move along that disparity at the right image to find the computed corresponding pixel. Then we compute the disparity in the left image for that pixel and see if we recover our original pixel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Siamese Networks**\n",
    "\n",
    "We would like to learn similarity metrics from data because hand crafted features do not take into consideration relevant geometric and radiometric invariance or occlusion patterns. Zbontar and Lecun (2016) showed that the computation of matching cost can be treated as an image classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method assumes a large disparity dataset and trains a CNN patch-wise based on images with ground truth disparity maps. Once we have trained a patch classifier, we can calculate features for both images and correlate features with a using a dot product or MLP and take a global \"winner takes all\" approach. Then a global optimization algorithm is run that incorporates some smoothness assumptions about the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the original paper there were two proposed architectures for this Siamese Network architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *learned similarity* method starts with one CNN with ReLU nonlinearities in parallel on both the left and right input patches and the features from both halves are then passed into a fully connected network trained to output a similarity score. This is a potentially more expressive solution than the one below, but it is very slow due to the MLP at the end.\n",
    "\n",
    "The *cosine similarity* method eliminates the fully connected network and applies a dot product to obtain the similarity score. This makes the algorithm much faster with no appreciable drop in performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training set is composed of patch triplets $(w_L(x_L), w_R(x_R^{\\verb|neg|}), w_R(x_R^{\\verb|pos|})$ where $w_L(x_L)$ is a patch from the left image centered at $x_L$, $w_R(x_R)$ is a patch from the second image centered at $x_R$, and the negative example is a patch centered at $(x_L - d + o, y_L)$ where $o$ is an offset drawn uniformly on $[-N_h, -N_l] \\cup [N_l, N_h]$. The positive examples are similarly patches centered at $(x_L - d + o_+, y_L)$, where $o_L$ is drawn uniformly from $[-P_h, P_h]$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically $P_h = 1$, $[N_l, N_h] = [3,6]$ so that negative examples are quite close to positive examples but still far enough away."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a Hinge Loss function $\\ell = \\max(0, m + s_- - s_+)$ where $s_+$ and $s_-$ are the scores for the positive and negative examples respectively. The loss is zero when the similarity of the positive example is greater than the similarity of the negative example by a margin at least $m$. This ensures that further separation of well-separated examples is not pursued and allows the model to focus on the hard cases. \n",
    "\n",
    "The margin $m$ is a tunable hyperparameter, but generally $m = 0.2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Spatial Regularization**\n",
    "\n",
    "We would also like to incorporate global optimization in addition to locally picking a matching patch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The underlying assumption in matching is that corresponding regions in both images should look similar and non-corresponding regions should look different. But this constraint can fail in some cases like:\n",
    "\n",
    "* surfaces without texture,\n",
    "* occlusions, and\n",
    "* non-lambertian surfaces.\n",
    "\n",
    "Such local ambiguities cannot be overcome by only using spatial features. So we need to use some form of global information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the real world, for example the Brown range image database, we see that depth varies slowly except at discontinuities which are sparse.\n",
    "\n",
    " We incorporate this constraint into the disparity estimation process by specifying a loopy markov random field (MRF) on a grid and solving for the whole disparity map $D$ at once. The MAP solution is then a minimum energy solution since we have $$p(D) = k \\exp \\left(- \\sum_{i} \\psi_{data}(d)i)  - \\lambda \\sum_{i \\sim j} \\psi_{smooth}(d_i, d_j)\\right)$$ where $k$ is some constant. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the terms mean:\n",
    "\n",
    " * $i \\sim j$ means the pixels are neighboring on a 4-connected grid\n",
    " * $d$ is the unknown disparity hypothesis\n",
    " * The Unary terms are the matching costs $\\psi_{data}(d)$\n",
    " * The pairwise terms represent the smoothness between pixels. For example, two option include $\\psi_{smooth}(d,d') = \\mathbb{1}_{d\\neq d'}$, and a truncated $\\ell_1$ function $\\psi_\\text{smooth}(d,d') = \\min(\\lvert d-d'\\rvert, \\tau)$.\n",
    "\n",
    "We can solve this MRF approximation using the belief propogation algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extend this idea of using Markov random fields to model more global relationships, and not just pairwise relationships. For example, disparities and objects can be modelled jointly to give constraints that span larger distances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **End-to-End Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These algorithms directly take images as input and output a full disparity map. This requires a lot of training data and could only work after emergence of datasets in this direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **DispNet**\n",
    "\n",
    "Is the first end-to-end model for stereo. It used a U-Net like architecture with skip-connections to retain details. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function was a Multi-scale loss (disparity error in pixels) and the loss function was applied even for the downscaled versions of the ground truth at intermediate layers. *Curriculum Learning* was used. Here the network is first trained on easy examples before the difficulty of the datasets was increased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating datasets of 3D imaginary with ground truth is difficult, and the Mayer et. al (2016) paper was pretrained on large synthetic datasets with cheap annotations and then fine tuned on a little real data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **GC-Net**\n",
    "\n",
    "A follow up work used a similar idea to DispNet but had improved performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key idea was to calculate the disparity cost volume and *3D convolutions* on it. This is very memory intensive and so small batch sizes should be used.\n",
    "\n",
    "It also converted the learned matching cost $c_\\theta(d)$ to a disparity via the expectation $$d^* = \\mathbb{E}[d] = \\sum_{d=0}^{D} \\verb|softmax|(-c_\\theta(d)) d$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_of_d = torch.tensor([1.0,3.0,10.0,3.0,1.0])\n",
    "d = torch.arange(*c_of_d.size())\n",
    "torch.sum(d * F.softmax(c_of_d, dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.0000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_of_d = torch.tensor([10.0,2.0,1.0,2.0,10.0])\n",
    "d = torch.arange(*c_of_d.size())\n",
    "torch.sum(d * F.softmax(c_of_d, dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Stereo Mixed Density Networks (SMD-Nets)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the intrinsic smoothness properties of neural networks we see a bleeding at the edges (pixels). SMD-Nets predict sharper boundaries at higher resolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They do so by predicting a bimodel (Laplacian) *mixture distribution* which allows accurate capture of uncertainty close to depth discontinuities. Another innovation was an MLP head that regresses the distribution parameters from interpolated features. This enables  training and inference at arbitrary spacial resolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Lecture 5: Probabilistic Graphical Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next few lectures are an excursion to techniques that are no longer dominating the field of computer vision but are still relevant today."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Structured Prediction**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fundamental assumption in block matching is that corresponding patches should look similar, but there are cases in which this assumption fails like textureless surface, non-lambertian surfaces, etc. To overcome this ambiguity we use prior knowledge. Depth in real images varies slowly except at object discontinuities. This is called *spatial regularization* and refers to the hypothesis that adjacent pixels should generally have similar disparity values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probabilistic graphical models take a probabilistic view and model the *dependency structure* of the problem via graphs. Structured predictions are made based on local constraints between adjacent random variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphical models were very popular before deep learning took over but they are still useful in the presence of little training data to integrate prior knowledge. The ideas involved in graphical model can be combined with and inform deep learning. We will see that we combine deep local features with inference in a graphical model and also develop deep local architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The advantages of graphical models are that they allow integration of prior knowledge, they require few parameters, and they are interpretable by design. However many phenomena are hard to model with complex dependencies structures that cannot be easily simplified. Large datasets cannot be exploited effectively with PGMs and inference on even smaller problems is only approximate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In structured prediction we are trying to learn a function $f : \\mathcal{X} \\to \\mathcal{Y}$ where the outputs $y \\in \\mathcal{Y}$ are complex (structured objects) like images, texts, parse trees, protein folds, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Markov Random Fields**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A *clique* in a graph is a fully connected subgraph of a graph and a *maximal clique* is a clique that cannot be extended by another vertex. A *potential* $\\phi(x_1,\\ldots,x_D)$ is a non-negative function of the set of variables $\\{x_1,\\ldots,x_D\\}$. A **Markov Random Field** is a product of potentials over the (maximal) cliques $\\{\\mathcal{X}_c\\}_{c =1}^C$ of a specified undirected graph $\\mathcal{G}$. We have $$p(\\mathcal{X}) = \\frac{1}{Z} \\prod_{c=1}^{C} \\phi_c(\\mathcal{X}_c)$$ where $Z$ is a normalizing constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A subset $\\mathcal{S}$ separates $\\mathcal{A}$ from $\\mathcal{B}$ in a graph $\\mathcal{G}$ if every path from $\\mathcal{A}$ to $\\mathcal{B}$ in $\\mathcal{G}$ passes through $\\mathcal{S}$. The **global markov property** asserts that for any $\\mathcal{S}$ that separates $\\mathcal{A}$ from $\\mathcal{B}$ we have $\\mathcal{A} \\scriptsize\\coprod  \\normalsize\\mathcal{B} \\mid \\mathcal{S}$. The **local markov property** states that when conditioning on the neighbors of a vertex $x$, the vertex is independent of the rest of the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key property is the *Hammersley--Clifford Theorem* which states that a probability distribution with a strictly positive mass or density function satisfies the markov properties with respect to an undirected graph $\\mathcal{G}$ if and only if it is a Gibbs random field, i.e its density can be factorized over the (maximal) cliques of the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Factor Graphs**\n",
    "\n",
    "While we may perform inference on MRFs, factor graphs are a bit more precise for this purpose.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a graph that is a 3-clique we have two factorizations $p(a,b,c) = \\frac{1}{Z} \\phi(a,b)\\phi(b,c)\\phi(c,a)$ and $p(a,b,c) = \\frac{1}{Z} \\phi(a,b,c)$. To remove this ambiguity we turn our graph into a bipartite graph with a separate type of (square) node for each factor. This allows us to explicitly model the factorization type over the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given $\\mathcal{X} = \\{x_1,\\ldots, x_D\\}$, cliques $\\mathcal{X}_k \\subseteq \\mathcal{X}$ and a function $f$ such that $$\\mathcal{f}(\\mathcal{X}) = \\prod_{k=1}^{K} f_k(\\mathcal{X}_k)$$ the corresponding **factor graph** is a bipartite graph with a square node for each factor $f_k$ and a square node for each variable $x_i$. By normalizing $f$ we obtain a distribution $$p(\\mathcal{X}) = \\prod_{k=1}^{K} f_k(\\mathcal{X}_k)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Belief Propogation**\n",
    "\n",
    "We will look at the belief propagation algorithm for inference in certain structured factor graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that we have a chain factor graph $p(a,b,c,d) = \\frac{1}{Z} f_1(a,b)f_2(b,c) f_3(c,d) f_4(d)$. If we wanted to naively compute the marginal distribution of $ a$, it will require $N^{3}$ summations. But we have more structure here, so we can do this more efficiently. We may write $$p(a,b,c) = \\sum_{d} p(a,b,c) = \\frac{1}{Z} f_1(a,b)f_2(b,c) \\underbrace{\\sum_{d} f_3(c,d) f_4(d)}_{\\mu_{d \\to c}(c)} = \\frac{1}{Z} f_1(a,b)f_2(b,c)  \\mu_{D \\to C}(c)$$ By precomputing the inner sum $\\mu_{D\\to C}$ at each possible value $c$ we can massively reduce the number of computations required. Continuing on in this manner, we have $$ p(a,b) = \\frac{1}{Z}\\sum_{c} p(a,b,c) = \\frac{1}{Z}\\sum_{c} f_1(a,b)f_2(b,c) \\mu_{D \\to C}(c) = \\frac{1}{Z}f_1(a,b) \\underbrace{\\sum_{c}f_2(b,c) \\mu_{D \\to C}(c)}_{\\mu_{C \\to B}(b)} = \\frac{1}{Z}f_1(a,b)\\mu_{C \\to B}(b) $$ Finally we have $$p(a) = \\sum_{b} p(a,b) = \\frac{1}{Z} \\sum_{b} f_1(a,b) \\mu_{C \\to B}(b) = \\frac{1}{Z} \\mu_{B \\to A}(A)$$ This is a linear time algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose that we have a tree structured factor graph. Again we will attempt to formulate the computation of marginals using the message passing scheme. We then have the following formula for messages from factor variables $$\\mu_{f \\to X}(x) = \\sum_{\\{Y = y, \\, \\ldots\\} \\in \\mathcal{X}_f \\setminus X} f(x, y, \\ldots) \\prod_{Y \\leftrightarrow f, Y \\neq X} \\mu_{Y \\to f}(y)$$ Here we sum over all possible assignments of the neighbors of that factor except the target $X = x$ and multiply the factor evaluated at an assignment with messages from all those neighbors evaluated at that same assignment. For a message from a variable node we have $$\\mu_{X \\to f}(x) = \\prod_{g \\leftrightarrow X, g \\neq f} \\mu_{g \\to X}(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once computed, we can reuse messages. Further each marginal can be written as a function of messages and so we only need an algorithm to compute all the messages. This is the *sum-product algorithm* or the *belief propagation algorithm*. This assumes that the graph is singly connected (a forest), but it can be extended to loopy graphs if we do not require exactness and are satisfied with approximate solutions.\n",
    "\n",
    "The steps of the algorithm are:\n",
    "1. *Initialization*. The messages from the leaf node factors $\\mu_{f \\to X}$ are initialized to the factor $$\\mu_{f\\to X}(x) = f(x)$$ and the messages from leaf nodes $\\mu_{X \\to f}(x)$ are set to 1.\n",
    "2. *Variable to Factor messages*. Here we use the formula $$\\mu_{X \\to f}(x) = \\prod_{g \\leftrightarrow X, g\\neq f} \\mu_{g \\to X}(x)$$\n",
    "3. *Factor to variable messages*. Now we compute these messages using the formula $$\\mu_{f \\to X}(x) = \\sum_{\\{Y = y, \\, \\ldots\\} \\in \\mathcal{X}_f \\setminus X} f(x, y, \\ldots) \\prod_{Y \\leftrightarrow f, Y \\neq X} \\mu_{Y \\to f}(y)$$\n",
    "4. *Repeat until all messages are calculated*.\n",
    "5. *Calculate the desired marginals*. The marginals $p(x) = \\lambda \\prod_{f \\leftrightarrow X} \\mu_{f \\to X}(x)$. Since probabilities have to be normalized, we can easily determine the unknown scale factor $\\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In large graphs messages may become very small or big due to repeated problems. The solution is to instead work with log messages $\\lambda$ and then the formulas become $$\\lambda_{X \\to f}(x) = \\sum_{g \\leftrightarrow X, g \\neq f} \\lambda_{g \\to X}(x)$$ and $$ \\lambda_{f \\to X}(x) = \\log\\left(\\sum_{\\{Y = y, \\, \\ldots\\} \\in \\mathcal{X}_f \\setminus X} f(x, y, \\ldots) \\exp\\left[\\sum_{Y \\leftrightarrow f, Y \\neq X} \\lambda_{Y \\to f}(y)\\right]\\right)$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **The Max-Product Algorithm**\n",
    "\n",
    "The goal here is to determine the maximum a-posteriori assignment of variables $$x^* = \\arg\\max_x p(x)$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we use the factorization structure to distribute maximization to local computations. For a chain we have $$\\begin{align*} \\max p(a,b,c,d) &= \\max_{a,b,c,d} f_1(a,b) f_2(b,c) f_3(c,d) \\\\\n",
    "&= \\max_{a,b,c} f_1(a,b) f_2(b,c) \\underbrace{\\max_d f_3(c,d)}_{\\mu_{D \\to C}(c)} \\\\\n",
    "&=  \\max_{a,b} f_1(a,b) \\underbrace{ \\max_{B \\to C} f_2(b,c) \\mu_{D \\to C}(c)}_{\\mu_{C \\to B}(b)} \\\\\n",
    "&= \\max_{a} \\underbrace{\\max_b f_1(a,b) \\mu_{C \\to B}(b)}_{\\mu_{B \\to A}(a)} \\\\ &= \\max_{a} \\mu_{B \\to A}(a)\\end{align*}$$ This algorithm so far provides the optimal value, and we need to backtrack to find the optimal argument. We have $$\\begin{align*} a^* &= \\arg\\max_{a} \\mu_{B\\to A}(a) \\\\ b^* &= \\arg\\max_b f_1(a^*, b) \\mu_{C \\to B}(b) \\\\ c^* &= \\arg\\max_c f_2(b^*, c) \\mu_{D \\to C}(c) \\\\ d^* &= \\arg\\max_d f_3(c^*, d) \\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Loopy Belief Propagation**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may try to apply the belief propagation algorithm with the same message formulas for loopy graphs but we will not be able to guarantee exactness or even convergence in simple cases. However this is still works surprisingly well in computer vision. \n",
    "\n",
    "The message passing schedule can either be a random or fixed order. A popular choice is to pass all factor to variable messages and then all variable to factor messages and repeat N times. This can be run in parallel since the graph is bipartite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Lecture 6: Applications of Graphical Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Stereo Reconstruction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can specify a loopy MRF on a grid and solve for the whole disparity map $D$ at once so that the MAP solution corresponds to the minimum energy solution. So we have $$\\begin{align*} p(D) &= c \\cdot \\prod_{i} f_\\text{data}(d_i) \\prod_{i \\sim j} f_{\\text{smooth}}(d_i,d_j) \\\\&= c \\cdot \\exp \\left[- \\left(\\sum_{i} \\psi_{\\text{data}}(d_i) + \\lambda \\sum_{i \\sim j} \\psi_{\\text{smooth}}(d_i,d_j)\\right)\\right]\\end{align*}$$ Here $\\psi(d) = -\\log f(d)$ and so maximizing the term inside the exponential (ignoring the sign) leads to a minimum energy solution. The smoothness term can be the truncated $\\ell_1$ penalty $\\psi_\\text{smooth}(d,d') = \\min (\\lvert d- d'\\rvert, \\tau)$. This MRF can belief propagation or graph cuts etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Multi-View Reconstruction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will discretize the 3D volume into *voxels*. We will let $o_i$ be a voxel occupancy indicator variable which indicates whether that cube is occupied or not and we will associate a number $a_i$ indicating the brightness of the corresponding voxel. Given a ray $r$ centered at a camera center, we will let $\\mathbf{o}_r = \\{o_1^r,\\ldots, o_{N_r}^r\\}$ and $\\mathbf{a}_r = \\{a_1^{r}, \\ldots, a_{N_r}^r\\}$ denote the voxels traversed by the ray in order. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an object in 3D space the color of a particular pixel in a corresponding pixel is the color of the first occupied voxel along that ray. So we have $$I_r = \\sum_{i=1}^{N} o_i \\prod_{j=1}^{i-1} (1 - o_j) a_i$$ where $I_r$ is the intesity at pixel $r$. Note that $o_i \\prod_{j=1}^{i-1} (1-o_j)$ is non-zero exactly at the first occupied voxel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to formulate a probabilistic model and perform inference using that model to model uncertainty. We have $$p(\\mathbf{O},\\mathbf{A}) = \\frac{1}{Z} \\prod_{v \\in \\mathcal{V}} \\phi_v(o_v) \\prod_{r \\in \\mathcal{R}} \\underbrace{\\psi_r(\\mathbf{o}_r, \\mathbf{a}_r)}_{ray}$$ The unary potentials just incorporate simple prior occupancy knowledge $$\\phi_v(o_v) = \\gamma^{o_v} (1-\\gamma)^{1-o_v}$$ Since most voxels are empty, we have $\\gamma < 0.5$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('vision')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2e493a1a15d7e27fb4e49f83da841848447539036e41bf31e3db46ef82884d8e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
