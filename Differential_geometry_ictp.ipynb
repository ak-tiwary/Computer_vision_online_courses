{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Differential Geometry (ICTP) notes**\n",
    "\n",
    "These are notes from Claudio Arezzo's Differential Geometry lectures on Youtube."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Lecture 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the length of a  curve is independent of reparameterization, we wish to identify a canonical parameterization. Given a curves $\\alpha : I \\to \\mathbb{R}^3$ and $t_0 \\in I$ define $s : I \\to \\mathbb{R}$ given by $$s(t) = \\int_{t_0}^{t} \\lvert \\alpha'(t)\\rvert dt$$ Then we have $s'(t)=\\lvert \\alpha(t)\\rvert$ and so $s$ is a diffeomorphism onto its image if $\\alpha' \\neq 0$. Such curves are called *regular*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may now use $s$ to reparameterize $\\alpha$ as $\\beta(s) = \\alpha(t(s))$. Then $\\beta'(s) = \\alpha'(t(s))t'(s) = \\frac{\\alpha'(t)}{\\lvert \\alpha'(t)\\rvert}$ and so $\\beta$ has *unit speed*. \n",
    "\n",
    "We say that a curve $\\beta : I \\to \\mathbb{R}^3$ is parameterized by arc length (p.a.l) if $\\lvert \\alpha'(t) \\rvert = 1$. This parameterization is unique up to a choice of base point $t_0$ and a constant of integration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both a line and a circle are 1 dimensional manifolds but we would like to define an intrinsic quantity that allows us to distinguish these. This quantity is *curvature*. \n",
    "\n",
    "Say we have a curve $\\alpha$ p.a.l. Define $T(s) = \\alpha'(s)$ so that $T$ is the unit velocity vector. This means $T \\cdot T = 1$ so that  we get $T' \\cdot T = 0$. Define $k(s) = \\lvert T'(s) \\rvert$. This is called the curvature of $\\alpha$ at $s$. This is smooth if $T' \\neq 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will assume that $k > 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define $N(s) = \\frac{T'(s)}{k(s)}$, the *normal* vector and $B(s) = T(s) \\wedge N(s)$, the *binormal* vector. This positively oriented orthonormal frame $\\{T(s), N(s), B(s)\\} is called the *Frenet frame*. \n",
    "\n",
    "We now have $T'(s) = k(s) N(s)$ and $B(s) = T(s) \\wedge N(s)$. This means $$B'(s) = T'(s) \\wedge N(s) + T(s) \\wedge N'(s) = T(s) \\wedge N(s)$$ Thus $B' \\cdot T = 0$. But we also have $B' \\cdot B = 0$ since $\\lvert B \\rvert = 1$, so we must have $B'(s) = \\tau(s) N(s)$. This quantity $\\tau(s)$ is called the *torsion* at $s$. $\\tau$ is a smooth function since $\\tau = \\langle B', N \\rangle$.\n",
    "\n",
    "Since we have $\\langle N, T \\rangle = \\langle N, B \\rangle = 0$. Taking the derivative we get $$\\langle N',T \\rangle + \\langle N, T'\\rangle= \\langle N',B\\rangle + \\langle N, B'\\rangle = 0$$ Now this means $$\\begin{align*}\\langle N', T \\rangle + k(s) &= 0 \\\\ \\angle N', B \\rangle + \\tau(s) &= 0 \\end{align*}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we have:\n",
    "\n",
    "**Theorem** (Frenet's Formula) Given $\\alpha : I \\to \\mathbb{R}^3$ p.a.l  and curvature $k > 0$, we have $$\\begin{align*} T' &= kN \\\\ N' &= -kT - \\tau B \\\\ B' &= \\tau N\\end{align*}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we have associated to any curve two quantities, curvature and torsion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Examples*.\n",
    "\n",
    "1. If we have a line $\\alpha(t) = t\\mathbf{v} + \\mathbf{v}_0$, then $T = \\mathbf{v}$ so that $T' = 0$ and so $k = 0$.\n",
    "2. If we have a circle $\\alpha(t) = r\\left(\\cos \\frac{t}{r}, \\sin \\frac{s}{r}, 0\\right) + \\mathbf{C}$ (already parameterized by arc-length), then $$T = \\alpha'(t) = \\left(-\\sin\\frac{t}{r}, \\cos\\frac{t}{r}, 0\\right)$$ This means $$T' = \\left(-\\frac{1}{r}\\cos \\frac{t}{r},-\\frac{1}{r}\\sin \\frac{t}{r}, 0\\right)$$ so that $k(t) = \\frac{1}{r}$. Thus $$N(t) = \\left(-\\cos\\frac{t}{r}, -\\sin\\frac{t}{r}, 0\\right)$$ Then we can calculate the binormal $B(t)$ using the cross product of these two vectors, and we get $B(t) = (0,0,1)$. Thus $B' = 0$ and so $\\tau = 0$.\n",
    "3. If we have a helix $$\\alpha(t) = \\left(a \\cos \\frac{t}{\\sqrt{a^2 + b^2}}, a \\sin \\frac{t}{\\sqrt{a^2 + b^2}},  \\frac{bt}{\\sqrt{a^2 + b^2}}\\right)$$ (which is again already parameterized by arc length), then $$T(t) = \\alpha'(t) = \\left(-\\frac{a}{\\sqrt{a^2+b^2}}\\sin \\frac{t}{\\sqrt{a^2 +b^2}}, \\frac{a}{\\sqrt{a^2+b^2}}\\cos \\frac{t}{\\sqrt{a^2+b^2}}, \\frac{b}{\\sqrt{a^2+b^2}}\\right)$$ Thus, $$T'(t) = \\frac{a}{a^2+b^2}\\left(-\\cos \\frac{t}{\\sqrt{a^2+b^2}}, -\\sin\\frac{t}{\\sqrt{a^2+b^2}}, 0\\right)$$ We assume $a > 0$ for simplicity, and then $k = \\frac{a}{a^2+b^2}$. Further calculations in this manner will show $\\tau(t) = -\\frac{b}{a^2+b^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem**. A curve $\\alpha : I \\to \\mathbb{R}^3$ is planar if and only if the torsion $\\tau = 0$. A curve is a straight line if and only if $k = 0$.\n",
    "\n",
    "*Proof*. We have $\\tau = 0$ if and only if $B = v$ is constant. Now $\\langle \\alpha, B\\rangle' = \\langle T, B\\rangle + \\langle \\alpha, B'\\rangle = \\langle \\alpha B' \\rangle$. If $B$ is constant, the RHS is zero and hence $\\alpha$ is planar. If $\\alpha$ is planar, $\\alpha \\cdot v = 0$ so that $T \\cdot v = 0$. We are assuming $k > 0$ everywhere, so this also means $N \\cdot v = 0$. Thus, $N' \\cdot v = 0 = - k T \\cdot v - \\tau B \\cdot v = 0$ Hence $\\tau B \\cdot v = 0$. If $\\tau \\neq 0$, $v$ is orthogonal to all three elements of an orthonormal frame, so we must have $\\tau = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Lecture 3**\n",
    "\n",
    "We wish to show that curvature and torsion determines curves (with positive curvature) up to diffeomorphism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose $\\alpha : I \\to \\mathbb{R}^3$ p.a.l and $k > 0$. Consider the map $s \\to \\begin{pmatrix}T(s) \\\\ N(s) \\\\ B(s) \\end{pmatrix} \\in \\mathbb{R}^{9}$. Then the Frenet formula states that $$\\begin{pmatrix} T' \\\\ N' \\\\ B' \\end{pmatrix} = \\begin{bmatrix}  &k I_3  &  \\\\ -k I_3 & & -\\tau I_3 \\\\ & \\tau I_3 & \\end{bmatrix} \\begin{pmatrix} T \\\\ N \\\\ B \\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem** (The fundamental theorem of the local theory of plane curves). \n",
    "\n",
    "Given $k_0, \\tau_0 : I \\to \\mathbb{R}$ that are smooth and satisfying $k_0 > 0$, there is a curve $\\alpha : I \\to \\mathbb{R}^3$ with the specified curvature and torsion and this curve is unique upto isometries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Proof*. Consider the following ODE $x'(s) = A_0(s) x(s)$ where $$A_0(s) = \\begin{bmatrix}  & k_0(s) I_3  &  \\\\ -k_0(s) I_3 & & -\\tau_0(s) I_3 \\\\ & \\tau_0(s) I_3 & \\end{bmatrix} $$ Choose a vector $\\vec{a} = (\\mathbf{t_0}, \\mathbf{n_0}, \\mathbf{b_0}) \\in \\mathbb{R}^9$ such that $\\mathbf{t_0}, \\mathbf{n_0}, \\mathbf{b_0}$ form an positively oriented orthonormal basis of $\\mathbb{R}^3$.\n",
    "\n",
    "Let $f : I \\to \\mathbb{R}^9$ be a solution of the above differential equation with initial condition $\\vec{a}$ (which exists because the differential equation is linear) and write $f = (\\mathbf{t}, \\mathbf{n}, \\mathbf{b})$ with $\\mathbf{t}, \\mathbf{n}, \\mathbf{b} \\in \\mathbb{R}^3$. Let $C = [\\mathbf{t} | \\mathbf{n} | \\mathbf{b}] \\in \\mathbb{R}^{3\\times 3}$.\n",
    "\n",
    "We wish to prove that $\\{\\mathbf{t}, \\mathbf{n}, \\mathbf{b}\\}$ is always a positively oriented orthonormal basis. Consider the matrix $M = C^TC$. Then since $f$ satisfies the above ODE, we have $2M'(s) = A(s)M(s) - M(s)A(s)$ where $A(s) = \\begin{bmatrix} 0 & k_0 & 0 \\\\ -k_0 & 0 & -\\tau_0 \\\\ 0 & \\tau_0 & 0\\end{bmatrix}$ (this can be checked by a slightly tedious but straightforward calculation).\n",
    "\n",
    "Now we know $M(0) = 0$ but $M(s) = \\text{Id}_3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Rapid Review of Multivariable Calculus**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem** (Taylor's Theorem in $\\mathbb{R}$) If $f : \\mathbb{R} \\to \\mathbb{R}$ is $\\mathcal{C}^{k+1}$ then we may write $$f(x) = f(a) + \\frac{f'(a)}{2}(x-a) + \\ldots + \\frac{f^{(k)}(a)}{k!} (x-a)^k  + \\frac{f^{(k+1)}(\\xi)}{(k+1)!} (x-a)^{k+1}$$ where $\\xi$ is between $a$ and $x$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Proof*. By replacing $f$ with $f(x+a)$ we may assume $a = 0$.\n",
    "\n",
    "First write $$f(x) = f(0) + \\int_{0}^x f'(t_1) dt_1$$ Then we may again write $$ f(x) = f(0) + \\int_{0}^x f'(0) + \\int_0^{t_1} f^{(2)}(t_2)dt_2 dt_1 = f(0) + f'(0)x + \\int_{0}^{x}\\int_{0}^{t_1} f^{(2)}(t_2) dt_2dt_1$$ Suppose that we have $$f(x) = f(0) + f'(0)x + \\ldots + \\frac{f^{(k-1))}(0)}{(k-1)!} + \\int_{0}^{x} \\int_{0}^{t_1} \\ldots \\underbrace{\\int_{0}^{t_{k-1}} f^{(k)}(t_k) dt_k \\ldots dt_1}_{I_k}$$ Then again we may write $f^{(k)}(t_k) = f^{(k)}(0) + \\int_{0}^{t_{k}} f^{(k+1)}(t_{k+1}) dt_{k+1}$ and we have $$\\begin{align*}f(x) &= f(0) + f'(0)x + \\ldots + \\frac{f^{(k-1))}(0)}{(k-1)!} + \\int_{0}^{x} \\int_{0}^{t_1} \\ldots \\int_{0}^{t_{k-1}} f^{(k)}(t_k) dt_k \\ldots dt_1 \\\\&= f(0) + f'(0)x + \\ldots + \\frac{f^{(k-1))}(0)}{(k-1)!} + \\int_{0}^{x} \\int_{0}^{t_1} \\ldots \\int_{0}^{t_{k-1}} f^{(k)}(0) dt_k \\ldots dt_1  + I_{k+1} \\\\ &= f(0) + f'(0)x + \\ldots + \\frac{f^{(k-1))}(0)}{(k-1)!}x^{k-1} + \\frac{f^{(k)(0)}}{k!}x^k  + I_{k+1}\\end{align*}$$ \n",
    "\n",
    "So we only need to show that the above integral formulation for the error term can be written in the desired form. For this we may assume $x > 0$ without loss of generality (if $x < 0$ replace $f$ with $f(-x)$). We also replace $k+1$ by $k$ for the remainder of the proof. Now $f^{(k)}$ is bounded in a (closed) interval containing $0$ and $x$, so write $m \\leq f^{(k)}(z) \\leq M$ in this interval and choose $m,M$ optimally so $f^{(k)}$ attains these values. Then integrating this inequality $k$ times gives $m \\frac{x^k}{k!} \\leq I_k \\leq M \\frac{x^k}{k!}$.  But then by the IVT we have $\\frac{f^{(k)}(\\xi)}{k!} = I_k$, as desired. $\\tiny\\square$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem** (Taylor's theorem in $\\mathbb{R}^n$). \n",
    "\n",
    "If $F : \\mathbb{R}^n \\to \\mathbb{R}$, we have $$F(\\mathbf{x}) = F(\\mathbf{x}) + \\nabla F(\\mathbf{a}) \\cdot (\\mathbf{x}-\\mathbf{a}) + \\frac{1}{2}(\\mathbf{x}-\\mathbf{a})^T H_{f,\\mathbf{a}} (\\mathbf{x}-\\mathbf{a}) + \\sum_{\\lvert \\alpha\\rvert = 3} \\partial^\\alpha f(\\xi) \\frac{(\\mathbf{x}-\\mathbf{a})^\\alpha}{\\alpha!}$$ where $\\vec{x}^\\alpha = x_1^{\\alpha_1} \\ldots x_n^{\\alpha_n}$ and $\\partial^\\alpha f = \\frac{\\partial^{\\lvert \\alpha\\rvert} f}{\\partial x_1^{\\alpha_1} \\ldots \\partial x_n^{\\alpha_n}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Proof*. \n",
    "\n",
    "We may assume that $\\mathbf{a} = \\mathbf{0}$ as usual. Let $g(t) = F(t\\mathbf{x})$. Then we may use the one variable taylor's theorem, and we get $$g(1) = g(0) + g'(0) + \\frac{g^{(2)}(0)}{2!} + \\frac{g^{(3)}(t)}{3!}$$ Let us compute these derivatives. By the chain rule, $g'(t) = \\nabla F(t\\mathbf{x}) \\cdot \\mathbf{x}$ and so $g'(0) = \\nabla F(\\mathbf{0}) \\cdot \\mathbf{x}$. Using the chain rule again, $$g^{(2)}(t) =  \\frac{d}{dt} \\nabla F(t\\mathbf{x}) \\cdot \\mathbf{x}$$ But $$\\nabla F(t \\mathbf{x}) = \\left(\\frac{\\partial F}{\\partial x_1}(t\\mathbf{x}), \\ldots, \\frac{\\partial F}{\\partial x_n}(t \\mathbf{x})\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now $\\frac{d}{dt} F_{x_j}(t\\mathbf{x}) = \\nabla F_{x_j} (t\\mathbf{x}) \\cdot \\mathbf{x}$, and so $\\frac{d}{dt} \\nabla F(t\\mathbf{x}) = H_{f, t\\mathbf{x}}\\mathbf{x}$. This means that $$g^{(2)}(t) = \\mathbf{x}^T H_{f, t\\mathbf{x}} \\mathbf{x}$$ and so $g^{(2)}(0) = \\mathbf{x}^T H_{f, \\mathbf{0}} \\mathbf{x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now write $g^{(2)}(t) = \\sum_{i,j} \\frac{\\partial^2 F}{x_ix_j}(t \\mathbf{x}) x_i x_j$. This means that $g^{(3)}(t) = \\sum_{i,j} x_ix_j\\frac{d}{dt} \\frac{\\partial^2 F}{x_ix_j}(t \\mathbf{x})$. So we need to compute $\\frac{d}{dt}F_{x_i,x_j}(t \\mathbf{x})$. This is $\\sum_{k=1}^{n} F_{x_k, x_i, x_j} (t \\mathbf{x}) x_k$. Thus we have $g^{(3)}(t) = \\sum_{i,j,k} F_{x_k, x_i, x_j}(t\\mathbf{x}) x_ix_jx_k$. By combining like $x_i,x_j,x_k$ terms and using basic combinatorics we have the desired result. $\\tiny\\square$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem** (Banach Fixed Point Theorem). Suppose that $X$ is a complete metric space and $F : X \\to X$ is a continuous contraction map. Then there is a unique fixed point for $F$ which can be found be repeated iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Proof*. Consider the sequence $\\{x_n\\}$ where $x_0$ is arbitrary and $x_{n+1} = F(x_n)$. Then $$d(x_{n+1}, x_n) \\leq C d(x_n, x_{n-1}) \\leq \\ldots \\leq C^{n} d(x_1,x_0)$$ Let $1 + d(x_1,x_0) = M$. Then $$\\begin{align*}d(x_{N+m}, x_{N+n}) &\\leq d(x_{N+m}, x_{N+m+1}) + \\ldots + d(x_{N+n-1}, x_{N+n}) \\\\\n",
    "&\\leq C^{N+m-1} M + \\ldots + C^{N + n-2} M \\\\\n",
    "&\\leq M(C^{N} + C^{N+1} + \\ldots) \\\\\n",
    "&= \\frac{MC^N}{1-C}\\end{align*}$$ Thus, if N is large enough so that $C^N < \\varepsilon \\cdot \\frac{1-C}{M}$, we have $d(x_{N+m}, x_{N+n}) < \\varepsilon$. Thus, $\\{x_n\\}$ is Cauchy and hence converges to some point $x_0$.\n",
    "\n",
    "To show uniqueness, let $\\{x_n\\}$ and $\\{y_n\\}$ be sequences formed in this manner converging to $x_0$ and $y_0$ respectively. Then, $$d(x_{n+1}, y_{n+1}) = d(F(x_n), F(y_n)) \\leq C d(x_{n}, y_n) \\leq C^n d(x_0,y_0)$$ Thus, $x_0 = y_0$, as desired (eventually $x_n$ is close to $x_0$ and to $y_n$ which is eventually close to $y_0$). $\\tiny\\square$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proposition**. If $G : \\mathbb{R}^n \\to \\mathbb{R}^m$ is a $\\mathcal{C}^1$ function, $DG$ is continuous with respect to the operator norm $\\lvert\\lvert\\cdot\\rvert\\rvert$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Proof*. Write $G = (g_1, \\ldots, g_m)$. We have $$\\begin{align*}\\lvert \\lvert DG_x \\rvert \\rvert &= \\sup_{\\lvert\\lvert y\\rvert\\rvert = 1} \\lvert\\lvert DG_x(y)\\rvert\\rvert_2  \\\\\n",
    "&= \\sup_{\\lvert\\lvert y\\rvert\\rvert = 1} \\lvert\\lvert DG_x(y)\\rvert\\rvert_2^2 \\\\ \n",
    "&= \\sup_{\\lvert\\lvert y\\rvert\\rvert = 1} \\sum_{i=1}^{m} \\sum_{j=1}^{n} \\left(\\frac{\\partial g_i(x)} {\\partial x_j} y_j\\right)^2\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose $z$ sufficiently close to $x$ so that each partial derivative at $x$ is within $\\epsilon$ of the corresponding partial derivative at $z$. Also choose $M > 0$ such that any partial derivative of $z$ (or any point sufficiently close to $x$) is at most $M$ in magnitude. Then we have $$ \\begin{align*} \\sum_{i=1}^{m} \\sum_{j=1}^{n} \\left(\\frac{\\partial g_i(x)} {\\partial x_j} y_j\\right)^2 &= \\sum_{i=1}^{m} \\sum_{j=1}^{n} \\left(\\left(\\frac{\\partial g_i(x)}{\\partial x_j}  - \\frac{\\partial g_i(z)}{\\partial x_j}\\right) y_j\\right)^2 \\\\ &+ \\sum_{i=1}^{m} \\sum_{j=1}^{n} \\left(\\frac{\\partial g_i(z)} {\\partial x_j} y_j\\right)^2 \\\\\n",
    "&+ \\sum_{i=1}^{m} \\sum_{j=1}^{n} 2y_j^2 \\frac{\\partial g_i(z)} {\\partial x_j}\\left(\\frac{\\partial g_i(x)} {\\partial x_j} - \\frac{\\partial g_i(z)} {\\partial x_j}\\right) \\end{align*}$$ This means that $$\\begin{align*} \\left\\lvert  \\sum_{i=1}^{m} \\sum_{j=1}^{n} \\left(\\frac{\\partial g_i(x)} {\\partial x_j} y_j\\right)^2 -  \\sum_{i=1}^{m} \\sum_{j=1}^{n} \\left(\\frac{\\partial g_i(z)} {\\partial x_j} y_j\\right)^2\\right\\rvert &\\leq \\epsilon^2 + 2M \\epsilon\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $y$ is chosen so as to maximize the quantity $A = \\lvert\\lvert D_x(y) \\rvert\\rvert^2$ we see that $B=\\lvert \\lvert D_z(y) \\rvert\\rvert^2 \\geq A - \\epsilon$. If $y$ is chosen to maximize $B$, then we have $A \\geq B -\\epsilon$. Thus, $\\lvert A - B \\rvert \\leq \\epsilon$. Hence $\\left\\lvert \\lvert\\lvert DG_x\\rvert\\rvert - \\lvert\\lvert DG_z\\rvert\\rvert \\right\\rvert \\leq \\epsilon$ if $z$ is sufficiently close to $x$. $\\tiny\\square$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemma**.  Suppose that $A$ is an $n\\times n$ matrix and $\\lvert\\lvert A  - I\\rvert\\rvert < 1$. Then $A$ is invertible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Proof*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem** (The Inverse Function Theorem). Suppose that $F : \\mathbb{R}^n \\to \\mathbb{R}^n$ is $\\mathcal{C}^k$ and $DF_{a}$ is invertible. Then $F$ is invertible in a neighborhood of $a$ and the inverse is also $\\mathcal{C}^k$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Proof*. Let $b = F(a)$. It is sufficient to assume $k=1$. Once we prove the result in this setting, partial derivatives of order $k$ of $F^{-1}$ will correspond to partial derivatives of order $k-1$ of the entries of $DF^{-1}_{b}$. But by Cramer's formula for the inverse of a matrix, such an entry is just a rational function of the entries of $DF_{a}$ where the denominator is non-vanishing. Since $F$ is of class $\\mathcal{C}^k$, such a function will have continuous partial derivatives of $k-1$ orders and so any rational function combination of these will as well (by using basic properties involving derivatives and algebra)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may assume that $a = b = 0$. Indeed, $G(x) = T_{-b}F(x+a)$ is invertible if and only if $F$ is and we have $DG_{0} = DF_{a}$. In fact, we may also assume $DF_{0} = I_{n}$, the identity matrix, because we may consider $G(x) = F(A^{-1} x)$ instead. Note that $G$ is invertible if and only if $F$ is, and $DG_{0}= I$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define $H(x) = x - F(x)$. Then $H$ is also $\\mathcal{C}^1$ and satisfies $H(0) = 0$ We also have $DH_0 = I - I = 0$. This means that $Dh$ is continuous in the operator norm and we have $\\lvert\\lvert Dh(x)\\rvert\\rvert_{\\text{op}} \\leq \\frac{1}{2}$ for any $x$ in a ball of sufficiently small radius $r$ about $0$. \n",
    "\n",
    "For $x \\in B(0,r)$ we have $\\lvert \\lvert I - DF_x\\rvert\\rvert \\leq \\frac{1}{2}$. This means that $DF_x$ is invertible for any $x$ in this neighborhood."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
