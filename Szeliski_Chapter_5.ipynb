{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Chapter 5. Deep Learning**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Supervised Learning**\n",
    "\n",
    "Supervised learning involves feeding pairs of inputs $\\mathbf{x}_i$ and the corresponding desired output values $y_i$ into a learning algorithm which adjusts the model's parameters so as to maximize the agreement between the model's predictions and the target outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is usually a good idea to center, standardize and if possible, *whiten* the input data. Whitening involves computing the covariance matrix of the input, takings its SVD, and then rotating the coordinate system so that the final dimensions are uncorrelated and have unit variance (under a Gaussian model). This is a computationally expensive process, but it can be quite practical and helpful for low-dimension inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Naive Bayes and Linear Discriminant Analysis**\n",
    "\n",
    "If we have a simple machine learning problem, or we can gather enough samples, we can determine the probability distribution of feature vectors for each class $p(\\mathbf{x} \\mid \\mathcal{C_k})$ as well as prior likelihoods $p(\\mathcal{C}_k)$. Using this, we can estimate the probabilities $p(\\mathcal{C}_k \\mid \\mathbf{x})$. In the case where the components of the feature vector are generated independently, $$p(\\mathbf{x} \\mid \\mathcal{C}_k) = \\prod_{i} p(x_i \\mid \\mathbf{C}_k)$$ the resulting technique is called a naive Bayes classifier.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the generative distributions are Gaussians with identical covariance matrices $\\Sigma$, we have $$p(\\mathbf{x} \\mid \\mathcal{C}_k) = \\frac{1}{(2\\pi)^{D/2} \\lvert \\Sigma\\rvert^{1/2}} \\exp\\left(-\\frac{1}{2}(\\mathbf{x}-\\mathbf{\\mu}_k) \\Sigma^{-1} (\\mathbf{x} - \\mathbf{\\mu}_k)\\right)$$ This means that the log-likelihood is of the form $$C - \\frac{1}{2}\\mathbf{x}^T\\Sigma^{-1}\\mathbf{x} + (\\Sigma^{-1}\\mu_k)^T\\mathbf{x}$$ The first term is just a constant, so we would just want to consider maximums of terms of the form $\\ell_k(\\mathbf{x}) = \\mathbf{w}_k^T\\mathbf{x} $. Since the decision boundaries are linear, this technique is known as *Linear Discriminant Analysis*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Logistic Regression**\n",
    "\n",
    "Logistic regression is a simple example of a *discriminative model* since it doesn't construct or assume a prior distribution over unknowns. This means we cannot generate random samples from the class using this model. Logistic regression does have some limitations, so it is often used only for the simplest classification tasks. If the classes in the feature space are not linearly separable, *kernel methods* can be a superior alternative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Support Vector Machines**\n",
    "\n",
    "Support vector machines (SVMs) find the linear decision boundary that maximizes the *margin* to the nearest training examples. We do this by finding the weight and bias $(\\mathbf{w},b)$ for which all the regression values $\\hat{y}_i = \\mathbf{w} \\cdot \\mathbf{x}_i + b$ satisfy $y_i \\cdot \\hat{y}_i \\geq 1$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If two classes are not linearly separable, we can replace linear regression with kernel regression. Rather than multiplying the weight vector $\\mathbf{w}$ with the feature vector $\\mathbf{x}$, we instead multiply it with kernel functions centered at the data point $\\mathbf{x}_k$. That is, the loss function $$\\ell_i = \\sum_{k} w_k \\phi(\\lvert \\lvert\\mathbf{x}_i - \\mathbf{x}_k\\rvert\\rvert) + b$$ However, instead of requiring the summation over all training samples $\\mathbf{x}_k$, once we solve for the maximum margin classifier, only a small subset of support vectors needs to be retained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In SVMs, we use the *Hinge Loss function* $\\mathcal{E}_{\\text{HL}}(\\hat{y}_i, y_i) = \\left(1 - y_i\\hat{y}_i\\right)_+$, and if we add the regularization term, we get the full loss function $$\\mathcal{E}_{\\text{SVM}}(\\mathbf{w}, b) = \\sum_{i} \\mathcal{E}_{\\text{HL}}(\\hat{y}_i, y_i) + \\lambda \\lvert\\lvert \\mathbf{w}\\rvert\\rvert^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hinge loss imposes no penalty on training samples that are on the correct side of boundary with sufficient margin, while the cross-entropy loss prefers larger absolute values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Decision Trees and Forests**\n",
    "\n",
    "In contrast to the above supervised learning techniques which process complete features at once, decision trees perform a sequence of simpler operations, often just looking at individual feature elements before deciding which element to look at next. A decision tree is constructed top-to-bottom by selecting decisions at each node that split the training samples that have made it to that node into more specific (lower entropy) distributions. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A *random forest* is created by building a set of decision trees, each of which makes slightly different decisions. At test time, a new sample is classified by each of the trees in the forest, and the class distributions at the final leaf nodes are averaged to provide an answer that is more accurate than could be obtained with a single tree. Random forests have several design parameters which can be used to tailor their accuracy, generalization, and complexity: the depth of each tree, the number of tress, the number of samples examined at each node, etc.  Each tree only looks at a random subset $\\rho$ of the training examples, and so each tree ends up having different decision functions at each node. Thus, ensembles of trees lead to softer decision boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Unsupervised Learning**\n",
    "\n",
    "In some applications, we are only given a set of data which we wish to characterize and understand by looking for patterns, regularities, or typical distributions. One option is to group the data into sets based on similarities like vector distance. In statistics, this problem is known as *cluster analysis* and is a widely studied area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **K-means and Gaussian mixture models**\n",
    "\n",
    "K-means implicitly model the probability density as a superposition of spherically symmetric distributions and does not require any probabilistic reasoning or modeling. Instead, the algorithm is given the number of clusters $k$ it is supposed to find and is initialized by randomly sampling $k$ centers from the input feature vectors. It then iteratively updates the cluster center location based on the samples that are closest to each center."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In mixtures of Gaussians, each cluster center is augmented by a covariance matrix whose values are re-estimated from the corresponding samples. Instead of using nearest neighbors to associate input samples with cluster centers, a *Mahalonobis distance* is used $$d(\\mathbf{x}_i, \\mathbf{\\mu}_k, \\Sigma_k) = \\lvert\\lvert \\mathbf{x}_i - \\mu_k \\rvert\\rvert_{\\Sigma_k^{-1}} = (\\mathbf{x_i} - \\mathbf{\\mu}_k)^T\\Sigma_k^{-1} (\\mathbf{x_i} - \\mathbf{\\mu}_k)$$ where $\\mathbf{x}_i$ are the input samples, $\\mathbf{\\mu}_k$ are the cluster centers, and $\\Sigma_k$ are their covariance estimates. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Manifold Learning**\n",
    "\n",
    "In many cases, the data we are analyzing doesn't reside in a globally linear subspace, but instead on a lower-dimensional manifold. In this case, non-linear dimensionality reduction can be used. Since these systems extract lower-dimensional manifolds in a higher dimensional space, they are also known as manifold learning techniques. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Semi-supervised Learning**\n",
    "\n",
    "In many machine learning settings, we have a modest amount of accurately labelled data and a far larger set of unlabelled or less accurate data. We would like to use this larger dataset as it captures characteristics of our future inputs even without labels. This area of study is called *semi-supervised learning*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Deep Neural Networks**\n",
    "\n",
    "Deep learning pipelines take an end-to-end approach to machine learning, optimizing each stage of the processing by searching for parameters that minimize the training loss. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common normalization technique for training deep neural networks is *spectral normalization* where the largest singular value of the weight matrix in each layer is constrained to being $1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is common to interpret the final outputs of a network as  a probability distribution, so  we should ask whether it is wise to use such probabilities as a measure of confidence in a particular answer. If a network is properly trained and predicting answers with good accuracy, it is tempting to make this assumption. However, the training losses chosen for this purpose only encourage the network to maximize the probability-weighted correct answers, and do not, in fact encourage the network output to be properly *confidence calibrated*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss functions can also be used in an unsupervised setting. Suppose we are given inputs $\\mathbf{x}_i$ and pairwise indicator variables $t_{ij}$ that indicate whether two inputs are similar. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to determine an embedding $\\mathbf{v}_i$ for each input $\\mathbf{x}_i$ such that similar input pairs have similar embeddings and dissimilar inputs have different (large distance) embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example of *metric learning* and is a commonly used tool in machine learning. The losses used to encourage this process are collectively known as ranking losses and can be used to relate features from different domains such as text and images. One example is the *contrastive loss function*, which is defined as $$\\mathcal{E}_{\\text{CL}} = \\sum_{(i,j) \\in \\mathcal{P}} \\left(t_{ij} \\log L_{\\text{S}}(d_{ij}) + (1-t_{ij})\\log L_{\\text{D}}(d_{ij})\\right)$$ where $\\mathcal{P}$ is the set of all labelled input pairs, $t_{ij}$ is an indicator function testing for whether element $i$ and $j$ are similar, $L_{\\text{S}}$ and $L_{\\text{D}}$ are the similar and dissimilar loss functions, and $d_{ij} = \\lvert\\lvert \\mathbf{v}_i -\\mathbf{v}_j\\rvert\\rvert$  are the pairwise distance between paired embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contrastive and similar triplet losses can be used to learn embeddings for visual similarity search. They have also been used for unsupervised pre-training of neural networks. In this case, it is more common to use a different contrastive loss function, inspired by the cross-entropy function. Before computing the loss, the embeddings are all normalized to unit norm $\\lvert\\lvert \\mathbf{v}_i\\rvert\\rvert = 1$. Then, the following loss function is summed over all matching embeddings $$\\ell_{ij} = -\\log \\frac{\\exp\\left(\\frac{\\mathbf{v}_i \\cdot \\mathbf{v_j}}{\\tau}\\right)}{\\sum_{k} \\exp \\left(\\frac{\\mathbf{v}_i \\cdot \\mathbf{v_k}}{\\tau}\\right)}$$ The denominator is summed over non-matches as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $\\tau$ variable denotes the \"temperature\" and controls how tight the clusters will be; it is sometimes replaced with an $s$ multiplier parameterizing the hyper-sphere radius. This loss is referred to by many names: InfoNCE, NT-Xent (normalized temperature cross-entropy loss). Generalized versions of this loss are called SphereFace, CosFace, and ArcFace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To maintain a comparable variance in the activations of successive layers, we must take into account the *fan-in* of each layer, i.e, the number of incoming connections where activations get multiplied by weights. The variance in the initial weights in layer $l$ should be set to $\\frac{2}{n_l}$. This is called *He initialization*.\n",
    "\n",
    " Adam and AdamW are currently the most popular optimizers for deep networks, although even with all their sophistication, learning rates need to be set carefully (and probably decayed over time) to achieve good results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A max pooling layer acts like a switch that connects one of the inputs to the output unit, so during backpropogation, we only need to pass the error and derivatives down to this maximally active unit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of reducing the resolution of a network and then expanding it again is sometimes called a *bottleneck* and is related to auto-encoders. Residual networks were introduced in 2016 and the main technical innovation was the introduction of *skip connection* to allow information and gradients to flow around a set of convolutional layers. This allowed the network to learn the residuals (differences) between a set of incoming and outgoing activations. The ResNeXt system used grouped convolutions to slightly improve accuracy, densely connected CNNs added skip connections between non-adjacent convolution and/or pool blocks, and Squeeze-and-Excitation networks added global context (via global pooling) to each layer to obtain a noticeable increase in accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the earliest networks tailored for lightweight execution was MobileNets, which used *depth-wise convolutions* (a special case of grouped convolutions where the number of groups equals the number of channels). By varying the width and resolution multipliers, the network could be tuned along an accuracy vs size vs computational efficiency tradeoff. The follow-on MobileNetV2 system added an \"inverted residual structure\" where the shortcut connections were between bottleneck layers. ShuffleNet added a \"shuffle\" stage between grouped convolutions to enable channels in different groups to co-mingle. ShuffleNet V2 added a channel split operator and tuned the network architectures using end-to-end performance measures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While people often download and use pre-trained neural networks for applications, it is more common to *fine-tune* such networks on data more characteristic of the application. It is also quite common to replace the last few layers (the *head* of the network). Depending on the amount and quality of training data available, the head can be as simple as a linear model (SVM or logistic regression) or as complex as a fully connected or convolutional network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing intermediate and final results has always been an integral part of computer vision algorithm development, and is an excellent way to debug or refine results. How can we visualize what individual units in a deep network respond to? For the first layer in a network, the response can be read directly from the incoming weights for a given channel. We can also find the patches in the validation set that produce the largest responses across the units in a given channel. For deeper layers in a network, we can again find maximally activating patches in the input images. Once these are found, we can pair a *deconvolution network* with the original network to backpropagate feature activations all the way back to the image patch. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The central idea in *self-supervised learning* is to create a supervised *pretext task* where the labels can be automatically derived from unlabelled images. One example is asking the network to predict a subset of the information from the rest. Once pre-trained, the network can be then modified and fine-tuned on the final intended downstream task. Some pretext tasks that have been proposed for pre-training image classification networks include:\n",
    "\n",
    "* *Context prediction*: taking nearby patches of images and predicting their relative positoins.\n",
    "* *Context encoders*: in-painting one or more missing regions in an image.\n",
    "* *9-tile jigsaw puzzle*: rearranging the tiles back to their correct positions.\n",
    "* *Colorizing black and white images*.\n",
    "* *Rotating images by multiples of $90^\\circ$ to make them upright*.\n",
    "\n",
    "In addition to using single-image pretext tasks, many researchers have used video clips, since successive frames contain semantically related content. One way to use this information is to order video frames correctly in time. Another is to extend colorization to video with the colors in the first frame given, which encourages the network to learn semantic categories and correspondences. And since videos usually come with sounds, these can be used as additional cues in self-supervision (for example, by asking a network to align visual and audio signals)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **More Complex Models**\n",
    "\n",
    "While deep networks started off being used in 2D image understanding and processing applications, they are now also widely used for 3D data including medical images and video sequences. We can also chain a series of deep networks together in time by feeding the results from one time frame to the next. Deep neural networks are now also being applied to video understanding, three-dimensional volumetric models (such as occupancy maps created from range data), and volumetric medical images. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Three-dimensional CNNs**\n",
    "\n",
    "It is convenient to operate on a complete 3d volume at once and have *weight sharing* across the third dimension for all kernels, as well as multiple input and output feature channels at each layer. An early application of 3D convolutions was in processing video data. The single frame approach classifies each frame independently, depending purely on that frame's content. Like regular 2D CNNs, 3D CNN architectures can exploit different spatial and temporal resolutions, striding, and channel depths, but they can also be very computation and memory intensive. To counteract this, a two-stream SlowFast architecture was developed in 2019, where a slow pathway operates at a lower frame rate and combines features from a fast pathway with higher temporal sampling but fewer channels. Video processing networks can also be made more efficient using channel-separated convolutions, and neural architecture search.\n",
    "\n",
    "In addition to processing 3D gridded data such as volumetric density, implicit distance function, and video sequences, neural networks can be used to infer 3D models from single images. One approach is to predict per-pixel depth, and another is to reconstruct full 3D models represented using volumetric density. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **RNNs, LSTMs, Transformers**\n",
    "\n",
    "RNNs and LSTMs are often used for video processing, since they can fuse information over time and model temporal dependencies. They are also used for language modelling, image captioning, and visual question answering. They have also been used to merge multi-view information in stereo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To propagate information forward in time, RNN-like networks need to encode all of the potentially useful information in the hidden state being passed between time steps. If there is need to look further back or forward in time, *attention* mechanisms are used. Such networks store a list of *keys* and *values*, which can be probed with a *query* to return a weighted blend of values depending on the alignment between the query and each key. Attention can be used to look backward at the hidden states in previous time instances (*self-attention*), or to look at different parts of the image (*visual attention*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Transformer is a novel architecture that adds attention mechanisms to deep neural networks. In contrast to RNNs, which process input tokens one at a time, transformers can operate on the entire input sequence at once. In the years after being introduced, transformers became the dominant paradigm for many tasks in natural language processing (NLP), enabling the impressive results produced by BERT and GPT-3. Transformers then began seeing success when processing the natural language component and later layers of many vision and language tasks. More recently, they have gained traction even in pure computer vision tasks, even outperforming CNNs on several popular benchmarks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The motivation for applying transformers to computer vision is different from that of applying it to NLP. Whereas RNNs suffer from sequentially processing the input,convolutions do not have this problem. Instead, the problem with convolution has to do with their *inductive biases*, i.e, the default assumptions encoded into convolutional models. A convolution operation assumes that nearby pixels are more important than far away pixels, and only after several convolutional layers are stacked together does the receptive field grow large enough to attend to the entire image. This  spatial locality bias  has led to remarkable success across many aspects of computer vision, but as as datsets, models, and computational power grow by orders of magnitude, these inductive biases may become a factor inhibiting further progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fundamental component of a transformer iis *self-attention*, which is itself built out of applying attention to each of the N unit activations in a given layer in the network. Attention is often described using an analogy to the concept of hash-maps. Given a set of key-value pairs $(k_i, v_i)$ and a query $q$, a dictionary returns the value corresponding to the key that exactly matches the query. In neural networks, the key and query values are real-valued vectors, and so the corresponding operation returns a weighted sum of values, where the weights depend on the pairwise distances between a query and the set of keys. The distance mechanism used is the *scaled dot-product attention*, which involves taking the dot product between the query and key vectors, and scaling down by the square root of the dimension of these embeddings $$\\mathbf{y} = \\sum_{i} \\alpha\\left(\\frac{\\mathbf{q}\\cdot \\mathbf{k}_i}{D}\\right)\\mathbf{v_i} = \\text{softmax}\\left(\\frac{\\mathbf{q}^T\\mathbf{K}}{D}\\right)^T\\mathbf{V}$$ where $\\mathbf{K}$ and $\\mathbf{V}$ are the row-stacked matrices composed of the key and value vectors, respectively, and $\\mathbf{y}$ is the output of the attention operator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Attention and Transformers (NYU DL 2020)**\n",
    "\n",
    "Language models assign a probability density to text. We factorize the distribution using the chain rule $p(x_0,\\ldots, x_n) = p(x_0) \\cdot \\prod_{i=i}^n [p(x_i \\mid x_0,\\ldots, x_{i-1})]$. At a high level, all language models convert text using an embedding $E$, compute a dot product between the context vector and the embedding vector, and use softmax to model the desired probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A transformer has an input embedding, $N$ transformer blocks, and a softmax output layer. The core transformer block has two sub-layers. The second sub-layer is just a feed-forward network, and the first sub-layer is a multi-head attention module. These sub-layers are connected by residual connections and layer normalization. Layer normalization is important for training, and there are some subtleties in how it is implemented to make training faster in practice. Transformers share weights across each time step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer Networks facilitate long range dependencies, but they don't have vanishing or exploding gradients. There will be fewer training steps and no recurrence (so parallelism can be exploited). Attention is a neural architecture mimics the retrieval of a value $v_i$ for a query $q$ in a fuzzy or probabilistic way. The retrieved value is $\\sum_{i} \\text{s}(q, k_i) \\cdot v_i$ where $s$ is a similarity function. Some similarity functions include the dot product $s(q,k) = q\\cdot k$, a scaled version of the dot product $s(q,k) = \\frac{q \\cdot k}{\\sqrt{d}}$ (where $d$ is the embedding dimension), a generalized dot product $s(q,k) = q^TWk$, or an additive similarity $w_1 \\cdot q + w_2 \\cdot k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this, we apply a softmax layer to transform $[s_1,s_2,\\ldots] \\to [a_1,a_2,\\ldots]$, and take a weighted combination of the softmax outputs. The softmax ensures that the output is a convex  combination of the values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example is machine translation, where the query is the hidden vector for the current word, the keys, which are the same as the values, are the hidden vector for all the words, and the attention mechanism allows us to decide how much attention to pay to each word if we are currently looking at this word. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a transformer, we will feed $V,K,Q$ into a linear layer, compute a scaled dot product attention, concatenate the outputs, and feed this through another linear layer. This output is called multi-head attention. In masked multi-head attention some values are masked, to prevent them from being selected. \n",
    "\n",
    "We use a positional embedding that depends on the position of the word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have a sequence $\\{x_i\\}_{i=1}^t$ as a matrix $X = [x_1 \\mid \\ldots \\mid x_t] \\in \\mathbb{R}^{n \\times t}$ for $x_i \\in \\mathbb{R}^n$. Consider $h = Xa = a_1x_1 + \\ldots + a_tx_t$. Hard attention requires $\\lvert a\\rvert_0 = 1$, that is, $a$ is a one-hot encoded vector. In this case, attention is focused on a single vector. Soft attention requires $\\lvert\\lvert a\\rvert\\rvert_1 = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will have $a = \\text{soft(arg)max}_\\beta(\\mathbf{X}^T\\mathbf{x})$. In this manner, we will get attention vectors $\\{a_i\\}_{i=1}^{T}$ which can be stacked to form a matrix $A \\in \\mathbb{R}^{t\\times t}$. Now we can use these attention vectors to get the hidden vectors $\\{h_i\\}_{i=1}^t$, these can be written as a matrix $H$ which has shape $n \\times t$. So we have $H = XA \\in \\mathbb{R}^{n \\times t}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given an input $q$, we will have query $q = W_qx$, $k = W_kx$, and value $v = W_vx$. We will have $q, k \\in \\mathbb{R}^{d'}$ and $v \\in \\mathbb{R}^{d'}$. Given a set of vectors $\\{x_i\\}_{i=1}^t$, we get sets of queries, keys and values $Q,K,V \\in \\mathbb{R}^{d \\times t}$. \n",
    "\n",
    "Now we want to compare each query to every key and find the similarities. We can then use the softmax function to find similarity scores for each key. We do this by computing $$a = \\text{[soft](arg)max}_\\beta(K^Tq) \\in \\mathbb{R}^t$$ Then the hidden layer will be an appropriately weighted linear combination $h = Va$. We will choose $\\beta = \\frac{1}{\\sqrt{d}}$ so that we control the growth of $K^Tq$. For all our vectors, $H = VA \\in \\mathbb{R}^{T}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To parallelize further, we can compute the matrix product $$\\begin{bmatrix} q\\\\k\\\\v\\end{bmatrix} = \\begin{bmatrix} W_q \\\\ W_k \\\\ W_v \\end{bmatrix}x \\in \\mathbb{R}^{3d}$$ If we have *multihead* attention, then we will have $$\\begin{bmatrix} q^1\\\\ \\vdots\\\\ q^h \\\\ k^1 \\\\ \\vdots \\\\ k^h \\\\ \\vdots \\end{bmatrix} = \\begin{bmatrix} W_q^1\\\\ \\vdots\\\\ W_q^h \\\\ W_k^1 \\\\ \\vdots \\\\ W_k^h \\\\ \\vdots \\end{bmatrix}\\mathbf{x}$$ So we will get a final hidden vector of dimension $\\mathbb{R}^{3hd}$. We then use a matrix $W_h \\in \\mathbb{R}^{d \\times hd}$ to go back to $\\mathbb{R}^{d}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformers have encoders and decoders. The encoder portion will have a self-attention component followed 1-convolution. These will both be followed by residual connections and layer normalization. This will give the hidden vectors. Now the decoder architecture will be similar, except there will be a cross-attention layer between the self attention and the 1-convolution layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder summarizes the content of the input set. The decoder queries what is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import math\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('vision')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2e493a1a15d7e27fb4e49f83da841848447539036e41bf31e3db46ef82884d8e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
