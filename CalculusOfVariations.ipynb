{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Calculus of Variations**\n",
    "\n",
    "My notes while learning calculus of variations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Calculus of Variations - Jost and Li-Jost**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The calculus of variations is concerned with the construction of optimal shapes, states, or processes where the optimality criterion is given in the form of an integral involving an unknown function. The field also has deep and important connections with other fields of mathematics. For instance, in geometrically defined classes of objects, a variational principle often permits the selection of a unique optimal representative. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classical calculus of variations consists in minimizing expressions of the form $$I(u) = \\int_{a}^{b} F(t, u(t), \\dot{u}(t)) dt $$ where $F : [a,b] \\times \\mathbb{R}^d \\times \\mathbb{R}^d$ is given and $u : [a,b] \\to \\mathbb{R}^d$ is unspecified. Usually, $u$ is part of a class of admissible functions and has to satisfy some constraints as well. A common constraint is the Dirichlet boundary condition $u(a) = u_1$ and $u(b) = u_2$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some examples of variational problems are:\n",
    "\n",
    "1. Minimizing the arc-length of the graph of a function $[a,b] \\to \\mathbb{R}$. This is the length of the curve $(t,u(t))$ among all graphs with prescribed boundary values $u(a)$, $u(b)$. This can be expressed as the minimization of the integral $$ \\int_{a}^{b} \\sqrt{1 + \\dot{u}^2} dt$$ This can easily be shown to be just the straight line between $u(a)$ and $u(b)$, i.e satisfying $\\ddot{u} = 0$.\n",
    "2. In the *Brachistochrone problem*, one wants to connect two points $(t_0, y_0)$ and $(t_1,y_1)$ by a curve in $\\mathbb{R}^2$ such that a particle obeying Newton's law of gravitation and moving without friction travels the distance between those points in the fastest possible way. After falling the height $y$, the particle has speed $\\sqrt{2gy}$, and so the time needed to traverse the path $y = u(t)$ is $$I(u) = \\int_{t_0}^{t_1} \\sqrt{ \\frac{1 + \\dot{u}^2}{2g \\cdot u}} dt$$\n",
    "3. A generalization of the above two problems is $$I(u) = \\int_{a}^{b} \\frac{\\sqrt{1 + \\dot{u}^2}}{\\gamma(t, u)}dt $$ where $\\gamma : [a,b] \\times \\mathbb{R} \\to \\mathbb{R}$ is a given positive function. This variational problem arises from Fermat's principle that a light ray chooses the path that needs the shortest time to be traversed among all possible paths. If the speed of light in a given medium is $\\gamma(t,u(t))$, we obtain the above problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $f : \\Omega \\to \\mathbb{R}$ where $\\Omega \\subset \\mathbb{R}^d$ is open, then to find a minimum of $f$ we would solve $Df(z_0) = 0$ and $D^2f(z_0) \\succeq 0$. We presently have a functional $I$ on the class of functions, but nevertheless, we expect that the first derivative of $I$ (something that remains to be defined), needs to vanish at a minimizer. Further, a suitable defined second derivative needs to be positive (semi)definite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume $F \\in \\mathcal{C}^1$ and that we have a minimizer of $I$ that is also $\\mathcal{C}^1$. Suppose also a Dirichlet boundary condition $u(a) = u_1$ and $u(b) = u_2$. Suppose that $\\eta : [a,b] \\to \\mathbb{R}^d$ is such that $\\eta$ is continuously differentiable and there is $a < a_1 \\leq b_1 < b$ with $\\eta(x) = 0$ for $x \\not\\in [a_1,b_1]$. Then $I(u + s \\eta) \\geq I(u)$. Let $f(s) = I(u + s\\eta)$. Now $$f(s) = I(u + s \\eta) = \\int_{a}^{b} F(t, u(t)  + s\\eta, \\dot{u} + s \\dot{\\eta})dt$$ Since $F, u, \\eta$ are of class $\\mathcal{C}^1$, we may differentiate the above expression with respect to $s$ at $s=0$, and we get $$f'(0) = \\int_{a}^{b} \\left[F_u(t, u, \\dot{u}) \\cdot \\eta + F_p(t,u,\\dot{u})\\cdot \\dot{\\eta}\\right]dt$$ where $F_p$ is the partial derivative of $F$ with respect to the third component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But $f(0)$ is a minimum of $f$, so $f'(0) = 0$. So, $$ \\int_{a}^{b} \\left[F_u(t, u, \\dot{u}) \\cdot \\eta + F_p(t,u,\\dot{u})\\cdot \\dot{\\eta}\\right]dt = 0$$ for all such $\\eta$. Now we assume that $F, u$ are in fact of class $\\mathcal{C}^2$. Then we may integrate the above equation by parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $U(t) = F_p(t, u(t), \\dot{u}(t))$ and $V(t) = \\eta(t)$. Then $$ \\int_{a}^{b} F_p(t,u,\\dot{u}) \\, \\dot{\\eta}\\, dt = \\int_{a}^{b} U dV = [U V]_{t=a}^{t=b} - \\int_{a}^{b} \\frac{d}{dt} F_p(t, u, \\dot{u}) \\,\\eta(t)\\, dt$$ But $V(b) = 0 = V(a)$, so we have $$ \\int_{a}^{b} \\left(F_u(t, u, \\dot{u}) - \\frac{d}{dt} F_p(t, u, \\dot{u})\\right) \\cdot \\eta(t)dt = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now invoke the *fundamental lemma of the calculus of variations*.\n",
    "\n",
    "**Lemma** If $h : (a,b) \\to \\mathbb{R}^d$ is continuous and satisfies $$\\int_{a}^{b} h(t)\\phi(t)dt = 0$$ for all $\\phi : (a,b) \\to \\mathbb{R}^d$ such that $\\text{supp}(\\phi) \\subset (a,b)$, then $h \\equiv 0$ on $(a,b).\n",
    "\n",
    "*Proof*. Suppose that $h(t_0) \\neq 0$. We may assume $h^1(t_0) \\neq 0$ (the first coordinate $h(t_0)$). Then there is a neighborhood $(c,d)$ of $t_0$ in $(a,b)$ such that $\\lvert h^1(x) \\rvert > \\frac{1}{2} \\lvert h^1(t_0)\\rvert$ for any $x \\in (c,d)$. Then we can choose an appropriate bump function $\\varphi^1(t)$ such that $\\varphi^1(t) = 0$ outside of $(c,d)$. Let $\\varphi(t) = (\\varphi^1(t), 0,\\ldots,0)$. Then $\\int_{a}^{b} h(t) \\varphi(t) dt = \\int_{c}^{d} h(t)\\varphi(t) dt \\neq 0$. $\\blacksquare$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So $$F_u(t, u, \\dot{u}) - \\frac{d}{dt} F_p(t, u, \\dot{u}) = 0$$ This system of equations is called the *Euler-Lagrange equations*. Expanding, we get $$F_{pp}(t, u, \\dot{u}) \\ddot{u} + F_{pu}(t,u, \\dot{u})\\dot{u} + F_{pt}(t,u,\\dot{u}) - F_u(t,u, \\dot{u}) = 0$$ This is a system of $d$ ordinary differential equations of second order that are linear in the second derivatives of the unknown function $u$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now consider the first problem from before:\n",
    "\n",
    "We have $$I(u) = \\int_{a}^{b} \\sqrt{1 + \\dot{u}^2} dt$$ So $F(t, u, \\dot{u}) = \\sqrt{1 + \\dot{u}^2}$. This means $F_u = 0$ and $F_p = \\frac{\\dot{u}}{\\sqrt{1 + \\dot{u}^2}}$. Now we have $F_p(x,y,z) = \\frac{z}{\\sqrt{1 + z^2}}$ so $$F_{pp}(z) = \\frac{\\sqrt{1 + z^2} - z\\frac{z}{\\sqrt{1+z^2}}}{1 + z^2} = \\frac{(1 + z^2) - z^2}{(1+ z^2)^{3/2}} = \\frac{1}{(1 + z^2)^{3/2}}$$ This means, the Euler Lagrange equations becomes $$ \\frac{1}{(1 + \\dot{u}^2)^{3/2}} \\cdot \\ddot{u} = 0 $$ This means $\\ddot{u} = 0$ and so $u$ is the straight line between $u(a)$ and $u(b)$.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
